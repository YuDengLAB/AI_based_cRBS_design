{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n",
    "    Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n",
    "    Concatenate, multiply, Flatten, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#define path\n",
    "bagan_gp_weight_path = \"./model/bagan_gp_step_weights/\"\n",
    "encoder_model_path = \"./model/seq_encoder_model/seq_encoder_model.h5\"\n",
    "decoder_model_path = \"./model/seq_encoder_model/seq_decoder_model.h5\"\n",
    "target_seq_path = \"./result/seq_result/\"\n",
    "\n",
    "##define dict for seq\n",
    "seq_dict={ 'T' : 0.25, 'C' : 0.5, 'A' : 0.75, 'G' : 1}\n",
    "inverse_seq_dict={ 0.25 : 'T', 0.5 : 'C', 0.75 : 'A', 1 : 'G'}\n",
    "\n",
    "#load encoder model\n",
    "encoder_model=load_model(encoder_model_path)\n",
    "decoder_model=load_model(decoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct BAGAN_GP model\n",
    "class BAGAN_GP(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        gp_weight=10.0,\n",
    "        trainRatio=3,\n",
    "    ):\n",
    "        super(BAGAN_GP, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.train_ratio = trainRatio\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(BAGAN_GP, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images, labels):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # get the interplated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator([interpolated, labels], training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            real_images = data[0]\n",
    "            labels = data[1]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        ########################### Train the Discriminator ###########################\n",
    "        # For each batch, we are going to perform cwgan-like process\n",
    "        for i in range(self.train_ratio):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            fake_labels = tf.random.uniform((batch_size,), 0, n_classes)\n",
    "            wrong_labels = tf.random.uniform((batch_size,), 0, n_classes)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator([random_latent_vectors, fake_labels], training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator([fake_images, fake_labels], training=True)\n",
    "                # Get the logits for real images\n",
    "                real_logits = self.discriminator([real_images, labels], training=True)\n",
    "                # Get the logits for wrong label classification\n",
    "                wrong_label_logits = self.discriminator([real_images, wrong_labels], training=True)\n",
    "\n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                d_cost = self.d_loss_fn(real_logits=real_logits, fake_logits=fake_logits,\n",
    "                                        wrong_label_logits=wrong_label_logits\n",
    "                                        )\n",
    "\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images, labels)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        ########################### Train the Generator ###########################\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        fake_labels = tf.random.uniform((batch_size,), 0, n_classes)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator([random_latent_vectors, fake_labels], training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator([generated_images, fake_labels], training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "# Build Discriminator without inheriting the pre-trained Encoder\n",
    "def discriminator_cwgan():\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    img = Input(img_size)\n",
    "    label = Input((1,), dtype='int32')\n",
    "\n",
    "\n",
    "    x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(img)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    le = Flatten()(Embedding(n_classes, 512)(label))\n",
    "    le = Dense(4 * 4 * 256)(le)\n",
    "    le = LeakyReLU(0.2)(le)\n",
    "    x_y = multiply([x, le])\n",
    "    x_y = Dense(512)(x_y)\n",
    "\n",
    "    out = Dense(1)(x_y)\n",
    "\n",
    "    model = Model(inputs=[img, label], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build discriminator with pre-trained Encoder\n",
    "def build_discriminator(encoder):\n",
    "\n",
    "    label = Input((1,), dtype='int32')\n",
    "    img = Input(img_size)\n",
    "\n",
    "    inter_output_model = Model(inputs=encoder.input, outputs=encoder.layers[-3].output)\n",
    "    x = inter_output_model(img)\n",
    "\n",
    "    le = Flatten()(Embedding(n_classes, 512)(label))\n",
    "    le = Dense(4 * 4 * 256)(le)\n",
    "    le = LeakyReLU(0.2)(le)\n",
    "    x_y = multiply([x, le])\n",
    "    x_y = Dense(512)(x_y)\n",
    "\n",
    "    out = Dense(1)(x_y)\n",
    "\n",
    "    model = Model(inputs=[img, label], outputs=out)\n",
    "\n",
    "    return model\n",
    "    \n",
    "def generator_label(embedding, decoder):\n",
    "    # # Embedding model needs to be trained along with GAN training\n",
    "    # embedding.trainable = False\n",
    "\n",
    "    label = Input((1,), dtype='int32')\n",
    "    latent = Input((latent_dim,))\n",
    "\n",
    "    labeled_latent = embedding([latent, label])\n",
    "    gen_img = decoder(labeled_latent)\n",
    "    model = Model([latent, label], gen_img)\n",
    "\n",
    "    return model\n",
    "\n",
    "def embedding_labeled_latent():\n",
    "    # # weight initialization\n",
    "    # init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    label = Input((1,), dtype='int32')\n",
    "    noise = Input((latent_dim,))\n",
    "    # ne = Dense(256)(noise)\n",
    "    # ne = LeakyReLU(0.2)(ne)\n",
    "\n",
    "    le = Flatten()(Embedding(n_classes, latent_dim)(label))\n",
    "    # le = Dense(256)(le)\n",
    "    # le = LeakyReLU(0.2)(le)\n",
    "\n",
    "    noise_le = multiply([noise, le])\n",
    "    # noise_le = Dense(latent_dim)(noise_le)\n",
    "\n",
    "    model = Model([noise, label], noise_le)\n",
    "\n",
    "    return model\n",
    "\n",
    "def decoder():\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    noise_le = Input((latent_dim,))\n",
    "\n",
    "    x = Dense(4*4*256)(noise_le)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    ## Size: 4 x 4 x 256\n",
    "    x = Reshape((4, 4, 256))(x)\n",
    "\n",
    "    ## Size: 8 x 8 x 128\n",
    "    x = Conv2DTranspose(filters=128,\n",
    "                        kernel_size=(4, 4),\n",
    "                        strides=(2, 2),\n",
    "                        padding='same',\n",
    "                        kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    ## Size: 16 x 16 x 128\n",
    "    x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    ## Size: 32 x 32 x 64\n",
    "    x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    ## Size: 64 x 64 x 3\n",
    "    generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init)(x)\n",
    "\n",
    "\n",
    "    generator = Model(inputs=noise_le, outputs=generated)\n",
    "    return generator\n",
    "\n",
    "latent_dim=128\n",
    "channel=1\n",
    "n_classes=5\n",
    "img_size=(64,64,1)\n",
    "trainRatio=5\n",
    "\n",
    "de = decoder()\n",
    "em = embedding_labeled_latent()\n",
    "\n",
    "d_model = discriminator_cwgan()  # without initialization\n",
    "g_model = generator_label(em, de)  # initialized with Decoder and Embedding\n",
    "bagan_gp = BAGAN_GP(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=latent_dim,\n",
    "    trainRatio=trainRatio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat seq api\n",
    "def min_distance(x):\n",
    "    y = 'T'\n",
    "    min_distance = abs(x-0.25)\n",
    "    for idx in inverse_seq_dict:\n",
    "        distance = abs(x-idx)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            y = inverse_seq_dict[idx]\n",
    "    return y\n",
    "\n",
    "def value_to_seq(value):\n",
    "    seq = []\n",
    "    for i in value:\n",
    "        sub_seq = []\n",
    "        for j in i:\n",
    "            sub_seq.append(min_distance(j))\n",
    "        seq.append(\"\".join(sub_seq))\n",
    "    return seq\n",
    "\n",
    "def use_bagan_create(num,target):\n",
    "## num: numbers of seq int32\n",
    "## target: class of seq int32 (0,1,2,3,4)\n",
    "    num_sample=num\n",
    "    latent_gen = np.random.normal(size=(num_sample, latent_dim))\n",
    "    decoded_imgs = bagan_gp.generator.predict([latent_gen, target*np.ones(num_sample)])\n",
    "    seq_value = decoder_model.predict(decoded_imgs)\n",
    "    seq_list = value_to_seq(seq_value)\n",
    "    return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create seq\n",
    "for i in range(20):\n",
    "    bagan_gp.load_weights(bagan_gp_weight_path + \"bagan_gp_weight_%d\" %(i)).expect_partial()\n",
    "    for j in range(5):\n",
    "        pre_seq=use_bagan_create(1024,j)\n",
    "        with open(target_seq_path + \"pre_seq_step{i}_class{j}.txt\".format(i=i+1,j=j+1),'w') as file:\n",
    "            for sub_seq in pre_seq:\n",
    "                file.write(sub_seq+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_seq_same(step_num,class_num):\n",
    "##return (no_repeat_num,count_same)\n",
    "#no_repeat_num:not repeat with themselves\n",
    "#count_same:repeat with training set\n",
    "    count_same=0\n",
    "    no_repeat_num=0\n",
    "    with open(target_seq_path+'pre_seq_step{step_num}_class{class_num}.txt'.format(step_num=step_num,\n",
    "                                                                                               class_num=class_num),\n",
    "              encoding='gbk') as f:\n",
    "        begin_txt=[]\n",
    "        for line in f:\n",
    "            begin_txt.append(line.strip())\n",
    "    no_repeat=[]\n",
    "    for i in begin_txt:\n",
    "        switch=True\n",
    "        for j in no_repeat:\n",
    "            if i==j:\n",
    "                switch=False\n",
    "        if switch:\n",
    "            no_repeat.append(i)\n",
    "    no_repeat_num=len(no_repeat)\n",
    "    with open('./data/raw_data/{class_num}.txt'.format(class_num=class_num),encoding='gbk') as f:\n",
    "        begin_txt=[]\n",
    "        for line in f:\n",
    "            begin_txt.append(line.strip())\n",
    "    next_txt=no_repeat\n",
    "    for i in next_txt:\n",
    "        for j in begin_txt:\n",
    "            if i==j:\n",
    "                count_same+=1\n",
    "                break\n",
    "    return (no_repeat_num,count_same)\n",
    "all_step_no_repeat=np.empty((20,5))\n",
    "all_step_count_same=np.empty((20,5))\n",
    "for i in range(20):\n",
    "    for j in range(5):\n",
    "        (num1,num2)=count_seq_same(i+1,j+1)\n",
    "        all_step_no_repeat[i][j]=num1\n",
    "        all_step_count_same[i][j]=num2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[982. 822. 895. 833. 852.]\n",
      " [962. 812. 858. 820. 809.]\n",
      " [983. 811. 884. 819. 825.]\n",
      " [977. 797. 856. 833. 790.]\n",
      " [975. 788. 888. 807. 804.]\n",
      " [977. 794. 867. 820. 809.]\n",
      " [958. 810. 857. 803. 826.]\n",
      " [965. 794. 878. 827. 816.]\n",
      " [983. 791. 892. 796. 788.]\n",
      " [975. 806. 872. 814. 821.]\n",
      " [979. 822. 865. 826. 802.]\n",
      " [964. 793. 898. 808. 836.]\n",
      " [961. 793. 889. 808. 830.]\n",
      " [973. 818. 894. 800. 790.]\n",
      " [970. 822. 877. 811. 823.]\n",
      " [978. 807. 878. 816. 809.]\n",
      " [975. 794. 896. 807. 823.]\n",
      " [969. 781. 903. 827. 821.]\n",
      " [969. 783. 894. 793. 809.]\n",
      " [964. 789. 889. 813. 784.]]\n"
     ]
    }
   ],
   "source": [
    "print(all_step_no_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237. 205. 124. 120. 137.]\n",
      " [264. 242. 147. 131. 177.]\n",
      " [274. 255. 143. 111. 181.]\n",
      " [299. 258. 147. 134. 186.]\n",
      " [306. 265. 157. 142. 174.]\n",
      " [299. 267. 158. 127. 172.]\n",
      " [313. 280. 140. 130. 187.]\n",
      " [312. 262. 160. 141. 195.]\n",
      " [299. 270. 152. 132. 173.]\n",
      " [318. 259. 144. 132. 186.]\n",
      " [306. 260. 136. 134. 174.]\n",
      " [292. 248. 142. 123. 176.]\n",
      " [310. 237. 152. 127. 177.]\n",
      " [286. 250. 141. 131. 172.]\n",
      " [301. 251. 142. 115. 180.]\n",
      " [294. 230. 157. 133. 172.]\n",
      " [300. 255. 145. 134. 168.]\n",
      " [292. 229. 154. 135. 175.]\n",
      " [300. 236. 154. 128. 176.]\n",
      " [292. 245. 146. 117. 169.]]\n"
     ]
    }
   ],
   "source": [
    "print(all_step_count_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[745. 617. 771. 713. 715.]\n",
      " [698. 570. 711. 689. 632.]\n",
      " [709. 556. 741. 708. 644.]\n",
      " [678. 539. 709. 699. 604.]\n",
      " [669. 523. 731. 665. 630.]\n",
      " [678. 527. 709. 693. 637.]\n",
      " [645. 530. 717. 673. 639.]\n",
      " [653. 532. 718. 686. 621.]\n",
      " [684. 521. 740. 664. 615.]\n",
      " [657. 547. 728. 682. 635.]\n",
      " [673. 562. 729. 692. 628.]\n",
      " [672. 545. 756. 685. 660.]\n",
      " [651. 556. 737. 681. 653.]\n",
      " [687. 568. 753. 669. 618.]\n",
      " [669. 571. 735. 696. 643.]\n",
      " [684. 577. 721. 683. 637.]\n",
      " [675. 539. 751. 673. 655.]\n",
      " [677. 552. 749. 692. 646.]\n",
      " [669. 547. 740. 665. 633.]\n",
      " [672. 544. 743. 696. 615.]]\n"
     ]
    }
   ],
   "source": [
    "print(all_step_no_repeat-all_step_count_same)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
